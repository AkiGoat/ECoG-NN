{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 100\n",
    "# NUM_CLASSES = 10\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/ecog_mel60_bi_dataset.pkl'\n",
    "with open(data_file, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=dataset[0],\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset[1],\n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BinarizedF(Function):\n",
    "#     def forward(self, input):\n",
    "#         self.save_for_backward(input)\n",
    "#         a = torch.ones_like(input)\n",
    "#         b = -torch.ones_like(input)\n",
    "#         output = torch.where(input>=0,a,b)\n",
    "#         return output\n",
    "#     def backward(self, output_grad):\n",
    "#         input, = self.saved_tensors\n",
    "#         input_abs = torch.abs(input)\n",
    "#         ones = torch.ones_like(input)\n",
    "#         zeros = torch.zeros_like(input)\n",
    "#         input_grad = torch.where(input_abs<=1,ones, zeros)\n",
    "#         return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=3, padding=1)\n",
    "#         self.mp = nn.MaxPool2d(3)\n",
    "        self.gap = nn.AvgPool2d(1)\n",
    "        self.fc = nn.Linear(120000, 384)\n",
    "#         self.fc2 = nn.Linear(120000, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        in_size = x.size(0)\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x1))\n",
    "        x3 = self.gap(x2)\n",
    "        x4 = x3.view(in_size, -1)\n",
    "        x5 = self.fc(x4)\n",
    "        with torch.no_grad():\n",
    "            x5 =x5\n",
    "#         x5 = torch.squeeze(x5)\n",
    "#         return (x5>0.5).float().requires_grad_()\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (gap): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=120000, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0623\n",
      "\n",
      "Test set: Average loss: 22.0313\n",
      "\n",
      "Epoch [2/100], Loss: 0.0554\n",
      "\n",
      "Test set: Average loss: 18.3926\n",
      "\n",
      "Epoch [3/100], Loss: 0.0468\n",
      "\n",
      "Test set: Average loss: 14.9152\n",
      "\n",
      "Epoch [4/100], Loss: 0.0387\n",
      "\n",
      "Test set: Average loss: 11.9964\n",
      "\n",
      "Epoch [5/100], Loss: 0.0310\n",
      "\n",
      "Test set: Average loss: 10.6345\n",
      "\n",
      "Epoch [6/100], Loss: 0.0280\n",
      "\n",
      "Test set: Average loss: 8.7203\n",
      "\n",
      "Epoch [7/100], Loss: 0.0202\n",
      "\n",
      "Test set: Average loss: 7.8437\n",
      "\n",
      "Epoch [8/100], Loss: 0.0201\n",
      "\n",
      "Test set: Average loss: 6.7273\n",
      "\n",
      "Epoch [9/100], Loss: 0.0170\n",
      "\n",
      "Test set: Average loss: 6.1926\n",
      "\n",
      "Epoch [10/100], Loss: 0.0162\n",
      "\n",
      "Test set: Average loss: 5.7654\n",
      "\n",
      "Epoch [11/100], Loss: 0.0138\n",
      "\n",
      "Test set: Average loss: 5.5885\n",
      "\n",
      "Epoch [12/100], Loss: 0.0151\n",
      "\n",
      "Test set: Average loss: 5.3643\n",
      "\n",
      "Epoch [13/100], Loss: 0.0144\n",
      "\n",
      "Test set: Average loss: 5.2467\n",
      "\n",
      "Epoch [14/100], Loss: 0.0123\n",
      "\n",
      "Test set: Average loss: 5.0464\n",
      "\n",
      "Epoch [15/100], Loss: 0.0119\n",
      "\n",
      "Test set: Average loss: 4.9990\n",
      "\n",
      "Epoch [16/100], Loss: 0.0117\n",
      "\n",
      "Test set: Average loss: 4.9429\n",
      "\n",
      "Epoch [17/100], Loss: 0.0120\n",
      "\n",
      "Test set: Average loss: 4.9199\n",
      "\n",
      "Epoch [18/100], Loss: 0.0110\n",
      "\n",
      "Test set: Average loss: 4.8559\n",
      "\n",
      "Epoch [19/100], Loss: 0.0119\n",
      "\n",
      "Test set: Average loss: 4.8168\n",
      "\n",
      "Epoch [20/100], Loss: 0.0121\n",
      "\n",
      "Test set: Average loss: 4.8107\n",
      "\n",
      "Epoch [21/100], Loss: 0.0105\n",
      "\n",
      "Test set: Average loss: 4.7633\n",
      "\n",
      "Epoch [22/100], Loss: 0.0124\n",
      "\n",
      "Test set: Average loss: 4.7371\n",
      "\n",
      "Epoch [23/100], Loss: 0.0105\n",
      "\n",
      "Test set: Average loss: 4.7395\n",
      "\n",
      "Epoch [24/100], Loss: 0.0114\n",
      "\n",
      "Test set: Average loss: 4.7223\n",
      "\n",
      "Epoch [25/100], Loss: 0.0108\n",
      "\n",
      "Test set: Average loss: 4.7093\n",
      "\n",
      "Epoch [26/100], Loss: 0.0112\n",
      "\n",
      "Test set: Average loss: 4.6978\n",
      "\n",
      "Epoch [27/100], Loss: 0.0105\n",
      "\n",
      "Test set: Average loss: 4.6729\n",
      "\n",
      "Epoch [28/100], Loss: 0.0122\n",
      "\n",
      "Test set: Average loss: 4.6778\n",
      "\n",
      "Epoch [29/100], Loss: 0.0115\n",
      "\n",
      "Test set: Average loss: 4.6500\n",
      "\n",
      "Epoch [30/100], Loss: 0.0116\n",
      "\n",
      "Test set: Average loss: 4.6501\n",
      "\n",
      "Epoch [31/100], Loss: 0.0120\n",
      "\n",
      "Test set: Average loss: 4.6406\n",
      "\n",
      "Epoch [32/100], Loss: 0.0119\n",
      "\n",
      "Test set: Average loss: 4.6398\n",
      "\n",
      "Epoch [33/100], Loss: 0.0112\n",
      "\n",
      "Test set: Average loss: 4.6222\n",
      "\n",
      "Epoch [34/100], Loss: 0.0100\n",
      "\n",
      "Test set: Average loss: 4.6281\n",
      "\n",
      "Epoch [35/100], Loss: 0.0110\n",
      "\n",
      "Test set: Average loss: 4.6238\n",
      "\n",
      "Epoch [36/100], Loss: 0.0114\n",
      "\n",
      "Test set: Average loss: 4.6182\n",
      "\n",
      "Epoch [37/100], Loss: 0.0109\n",
      "\n",
      "Test set: Average loss: 4.6123\n",
      "\n",
      "Epoch [38/100], Loss: 0.0120\n",
      "\n",
      "Test set: Average loss: 4.5823\n",
      "\n",
      "Epoch [39/100], Loss: 0.0103\n",
      "\n",
      "Test set: Average loss: 4.5736\n",
      "\n",
      "Epoch [40/100], Loss: 0.0114\n",
      "\n",
      "Test set: Average loss: 4.5776\n",
      "\n",
      "Epoch [41/100], Loss: 0.0099\n",
      "\n",
      "Test set: Average loss: 4.5909\n",
      "\n",
      "Epoch [42/100], Loss: 0.0116\n",
      "\n",
      "Test set: Average loss: 4.5858\n",
      "\n",
      "Epoch [43/100], Loss: 0.0107\n",
      "\n",
      "Test set: Average loss: 4.5995\n",
      "\n",
      "Epoch [44/100], Loss: 0.0109\n",
      "\n",
      "Test set: Average loss: 4.6023\n",
      "\n",
      "Epoch [45/100], Loss: 0.0114\n",
      "\n",
      "Test set: Average loss: 4.5714\n",
      "\n",
      "Epoch [46/100], Loss: 0.0111\n",
      "\n",
      "Test set: Average loss: 4.5957\n",
      "\n",
      "Epoch [47/100], Loss: 0.0110\n",
      "\n",
      "Test set: Average loss: 4.5747\n",
      "\n",
      "Epoch [48/100], Loss: 0.0103\n",
      "\n",
      "Test set: Average loss: 4.5614\n",
      "\n",
      "Epoch [49/100], Loss: 0.0115\n",
      "\n",
      "Test set: Average loss: 4.5942\n",
      "\n",
      "Epoch [50/100], Loss: 0.0104\n",
      "\n",
      "Test set: Average loss: 4.5657\n",
      "\n",
      "Epoch [51/100], Loss: 0.0113\n",
      "\n",
      "Test set: Average loss: 4.5554\n",
      "\n",
      "Epoch [52/100], Loss: 0.0103\n",
      "\n",
      "Test set: Average loss: 4.5780\n",
      "\n",
      "Epoch [53/100], Loss: 0.0105\n",
      "\n",
      "Test set: Average loss: 4.5731\n",
      "\n",
      "Epoch [54/100], Loss: 0.0113\n",
      "\n",
      "Test set: Average loss: 4.5573\n",
      "\n",
      "Epoch [55/100], Loss: 0.0097\n",
      "\n",
      "Test set: Average loss: 4.5741\n",
      "\n",
      "Epoch [56/100], Loss: 0.0101\n",
      "\n",
      "Test set: Average loss: 4.5582\n",
      "\n",
      "Epoch [57/100], Loss: 0.0099\n",
      "\n",
      "Test set: Average loss: 4.5689\n",
      "\n",
      "Epoch [58/100], Loss: 0.0095\n",
      "\n",
      "Test set: Average loss: 4.5587\n",
      "\n",
      "Epoch [59/100], Loss: 0.0103\n",
      "\n",
      "Test set: Average loss: 4.5370\n",
      "\n",
      "Epoch [60/100], Loss: 0.0113\n",
      "\n",
      "Test set: Average loss: 4.5425\n",
      "\n",
      "Epoch [61/100], Loss: 0.0111\n",
      "\n",
      "Test set: Average loss: 4.5406\n",
      "\n",
      "Epoch [62/100], Loss: 0.0109\n",
      "\n",
      "Test set: Average loss: 4.5436\n",
      "\n",
      "Epoch [63/100], Loss: 0.0098\n",
      "\n",
      "Test set: Average loss: 4.5729\n",
      "\n",
      "Epoch [64/100], Loss: 0.0106\n",
      "\n",
      "Test set: Average loss: 4.5572\n",
      "\n",
      "Epoch [65/100], Loss: 0.0086\n",
      "\n",
      "Test set: Average loss: 4.5506\n",
      "\n",
      "Epoch [66/100], Loss: 0.0098\n",
      "\n",
      "Test set: Average loss: 4.4869\n",
      "\n",
      "Epoch [67/100], Loss: 0.0086\n",
      "\n",
      "Test set: Average loss: 4.5505\n",
      "\n",
      "Epoch [68/100], Loss: 0.0097\n",
      "\n",
      "Test set: Average loss: 4.5080\n",
      "\n",
      "Epoch [69/100], Loss: 0.0095\n",
      "\n",
      "Test set: Average loss: 4.4707\n",
      "\n",
      "Epoch [70/100], Loss: 0.0093\n",
      "\n",
      "Test set: Average loss: 4.4549\n",
      "\n",
      "Epoch [71/100], Loss: 0.0089\n",
      "\n",
      "Test set: Average loss: 4.4735\n",
      "\n",
      "Epoch [72/100], Loss: 0.0093\n",
      "\n",
      "Test set: Average loss: 4.4614\n",
      "\n",
      "Epoch [73/100], Loss: 0.0098\n",
      "\n",
      "Test set: Average loss: 4.5184\n",
      "\n",
      "Epoch [74/100], Loss: 0.0086\n",
      "\n",
      "Test set: Average loss: 4.3675\n",
      "\n",
      "Epoch [75/100], Loss: 0.0092\n",
      "\n",
      "Test set: Average loss: 4.3553\n",
      "\n",
      "Epoch [76/100], Loss: 0.0087\n",
      "\n",
      "Test set: Average loss: 4.3957\n",
      "\n",
      "Epoch [77/100], Loss: 0.0092\n",
      "\n",
      "Test set: Average loss: 4.2493\n",
      "\n",
      "Epoch [78/100], Loss: 0.0091\n",
      "\n",
      "Test set: Average loss: 4.1763\n",
      "\n",
      "Epoch [79/100], Loss: 0.0087\n",
      "\n",
      "Test set: Average loss: 4.1232\n",
      "\n",
      "Epoch [80/100], Loss: 0.0084\n",
      "\n",
      "Test set: Average loss: 4.0667\n",
      "\n",
      "Epoch [81/100], Loss: 0.0078\n",
      "\n",
      "Test set: Average loss: 4.0432\n",
      "\n",
      "Epoch [82/100], Loss: 0.0077\n",
      "\n",
      "Test set: Average loss: 4.0761\n",
      "\n",
      "Epoch [83/100], Loss: 0.0070\n",
      "\n",
      "Test set: Average loss: 4.0545\n",
      "\n",
      "Epoch [84/100], Loss: 0.0066\n",
      "\n",
      "Test set: Average loss: 3.9428\n",
      "\n",
      "Epoch [85/100], Loss: 0.0077\n",
      "\n",
      "Test set: Average loss: 3.9751\n",
      "\n",
      "Epoch [86/100], Loss: 0.0069\n",
      "\n",
      "Test set: Average loss: 3.8704\n",
      "\n",
      "Epoch [87/100], Loss: 0.0068\n",
      "\n",
      "Test set: Average loss: 3.8818\n",
      "\n",
      "Epoch [88/100], Loss: 0.0067\n",
      "\n",
      "Test set: Average loss: 3.7915\n",
      "\n",
      "Epoch [89/100], Loss: 0.0069\n",
      "\n",
      "Test set: Average loss: 3.7345\n",
      "\n",
      "Epoch [90/100], Loss: 0.0068\n",
      "\n",
      "Test set: Average loss: 3.7894\n",
      "\n",
      "Epoch [91/100], Loss: 0.0069\n",
      "\n",
      "Test set: Average loss: 3.6929\n",
      "\n",
      "Epoch [92/100], Loss: 0.0060\n",
      "\n",
      "Test set: Average loss: 3.7004\n",
      "\n",
      "Epoch [93/100], Loss: 0.0056\n",
      "\n",
      "Test set: Average loss: 3.6319\n",
      "\n",
      "Epoch [94/100], Loss: 0.0064\n",
      "\n",
      "Test set: Average loss: 3.6634\n",
      "\n",
      "Epoch [95/100], Loss: 0.0063\n",
      "\n",
      "Test set: Average loss: 3.6555\n",
      "\n",
      "Epoch [96/100], Loss: 0.0062\n",
      "\n",
      "Test set: Average loss: 3.5910\n",
      "\n",
      "Epoch [97/100], Loss: 0.0060\n",
      "\n",
      "Test set: Average loss: 3.6119\n",
      "\n",
      "Epoch [98/100], Loss: 0.0061\n",
      "\n",
      "Test set: Average loss: 3.5854\n",
      "\n",
      "Epoch [99/100], Loss: 0.0058\n",
      "\n",
      "Test set: Average loss: 3.5880\n",
      "\n",
      "Epoch [100/100], Loss: 0.0058\n",
      "\n",
      "Test set: Average loss: 3.5446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    model.train()\n",
    "    for i, (ecog, mel) in enumerate(train_loader):\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "#         outputs = model(images).double()\n",
    "#         labels = labels.long()\n",
    "        outputs = model(ecog)\n",
    "        labels = mel\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i) % 100 == 0:\n",
    "            train_loss_list.append(loss.item())\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, NUM_EPOCH, loss.item()))\n",
    "#             model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "#             with torch.no_grad():\n",
    "#                 correct = 0\n",
    "#                 total = 0\n",
    "#                 for images, labels in test_loader:\n",
    "#                     images = images.to(device)\n",
    "#                     labels = labels.to(device)\n",
    "#                     outputs = model(images)\n",
    "#                     _, predicted = torch.max(outputs.data, 1)\n",
    "#                     total += labels.size(0)\n",
    "#                     correct += (predicted == labels).sum().item()\n",
    "\n",
    "#                 print('Test Accuracy of the model on the 250 test images: {} %'.format(100 * correct / total))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "#         data, target = Variable(data, volatile=True), Variable(target)\n",
    "#         data = data.to(device)\n",
    "#         target = target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        target = target\n",
    "        test_loss += F.l1_loss(output, target, size_average=False).data\n",
    "        # get the index of the max log-probability\n",
    "#         pred = output.data.max(1, keepdim=True)[1]\n",
    "#         correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_loss_list.append(test_loss.item())\n",
    "#     test_accuracy_list.append((100. * correct / len(test_loader.dataset)).item())\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(\n",
    "        test_loss))\n",
    "#             print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                    .format(epoch+1, NUM_EPOCH, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5899, 0.5404, 0.5082,  ..., 0.5543, 0.6250, 0.6580],\n",
      "         [0.5185, 0.4716, 0.4357,  ..., 0.4995, 0.5500, 0.5897],\n",
      "         [0.4580, 0.4169, 0.3887,  ..., 0.4618, 0.5055, 0.5173],\n",
      "         ...,\n",
      "         [0.4378, 0.4246, 0.3890,  ..., 0.4505, 0.5450, 0.6006],\n",
      "         [0.3707, 0.3522, 0.3064,  ..., 0.3600, 0.4493, 0.5129],\n",
      "         [0.3951, 0.3696, 0.3289,  ..., 0.3244, 0.3983, 0.4680]]])\n"
     ]
    }
   ],
   "source": [
    "img = dataset[1][1][0]\n",
    "img_arr = np.asarray([t.numpy() for t in img])\n",
    "img = Variable(torch.from_numpy(img_arr)).view(1,96,125)\n",
    "# img = Variable(img).view(1,96,125)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.3409e-03,  4.9698e-03,  4.0158e-03,  7.7791e-03, -2.0392e-03,\n",
      "         -2.2999e-03,  2.1482e-03, -2.9461e-03,  6.5758e-03,  1.1294e-02,\n",
      "          1.0148e-02,  8.7027e-04,  1.8357e-03, -4.5515e-03, -8.0420e-04,\n",
      "          8.0880e-03,  1.7540e-03, -5.5693e-03,  4.8609e-03,  2.6625e-03,\n",
      "          1.2221e-03, -2.6295e-05, -6.0669e-03,  2.1152e-03,  4.4100e-03,\n",
      "         -5.6866e-03, -8.5445e-03, -1.4103e-03, -2.3485e-03,  2.6079e-03,\n",
      "          4.3043e-03, -9.3719e-03, -1.1054e-03, -1.8285e-03,  5.8138e-03,\n",
      "         -3.7034e-03, -1.0881e-02,  2.1718e-03, -6.8471e-03,  2.6267e-03,\n",
      "         -7.7965e-03,  6.5234e-03,  2.2215e-03,  4.7486e-04, -7.3440e-03,\n",
      "          3.7997e-03, -7.4452e-03,  4.7899e-03, -1.8600e-03, -4.1764e-03,\n",
      "         -1.3390e-03,  5.6570e-04, -6.7148e-03, -4.1869e-03,  6.3286e-03,\n",
      "          2.5897e-03, -7.8124e-03, -2.9254e-03,  3.3301e-03, -8.1339e-03,\n",
      "          4.5755e-04,  6.2220e-04, -1.7682e-03, -8.5711e-03,  2.5575e-03,\n",
      "          2.6676e-03,  8.8095e-03,  4.5288e-03, -5.8299e-03, -1.5728e-03,\n",
      "          2.6803e-03, -2.2828e-05,  9.6841e-04, -3.4042e-03, -2.1087e-03,\n",
      "         -5.5793e-03,  1.4930e-02, -2.6004e-03,  1.4012e-01,  9.4516e-01,\n",
      "          9.5088e-01,  9.1183e-02,  5.2664e-01,  3.0325e-01, -2.5322e-03,\n",
      "          4.6887e-03, -2.5292e-03, -4.3952e-03, -1.1064e-03, -9.8394e-03,\n",
      "         -7.2907e-03,  2.8130e-02,  1.9780e-02, -9.9008e-03, -1.2119e-03,\n",
      "         -6.5638e-03,  8.3843e-04,  3.2620e-03, -4.4251e-03, -7.6012e-04,\n",
      "          1.1724e-03,  1.0524e-02,  2.1020e-03,  2.8375e-04,  1.1119e-02,\n",
      "          4.0391e-04,  5.0604e-03, -2.1417e-03,  3.6861e-03,  1.3931e-02,\n",
      "         -2.3702e-03,  5.8256e-02,  3.3645e-02,  2.1351e-03,  5.5166e-02,\n",
      "          4.6386e-02, -2.5833e-02,  1.4636e-02, -3.8272e-04,  1.0144e-03,\n",
      "         -7.2552e-03, -9.5998e-03,  2.2484e-03,  2.0280e-02,  2.6190e-02,\n",
      "          2.8609e-02,  1.3079e-02,  1.9432e-02,  7.5924e-04, -1.5803e-03,\n",
      "          2.7205e-03,  7.0521e-04,  2.3195e-03, -6.0221e-02, -1.8659e-02,\n",
      "         -2.7593e-02, -6.0594e-02, -6.3176e-02,  5.0611e-03, -1.0667e-04,\n",
      "         -4.0301e-03,  8.9623e-03,  3.0448e-02,  2.3835e-02,  3.1319e-02,\n",
      "          2.9179e-02,  5.0279e-02,  6.0557e-03, -5.0656e-04, -2.4032e-03,\n",
      "          8.1614e-03,  4.4397e-02,  4.0006e-02,  5.0808e-02,  5.1742e-02,\n",
      "          5.9801e-02, -5.5924e-03, -1.8330e-03, -9.9569e-04,  7.7111e-03,\n",
      "         -2.1923e-03, -5.6326e-03, -1.1673e-02,  7.0140e-03, -6.7882e-03,\n",
      "         -1.4991e-02, -2.1051e-02, -1.6838e-02,  2.1256e-02, -1.4154e-02,\n",
      "         -3.4692e-03,  4.2847e-04, -3.1244e-03, -1.4882e-03, -9.2381e-05,\n",
      "         -1.8013e-03,  3.1054e-03, -2.3351e-03, -6.4539e-03,  5.2958e-04,\n",
      "          2.0146e-03, -2.4525e-03, -9.4143e-03,  7.5344e-04,  1.0450e-03,\n",
      "         -5.6180e-03, -4.3361e-03,  7.4427e-03,  7.6361e-03,  1.4273e-03,\n",
      "         -1.2351e-03, -1.2418e-02, -1.3478e-03,  5.8566e-03, -4.3374e-03,\n",
      "         -4.0084e-04,  6.2465e-03, -5.6013e-03, -4.8546e-03, -4.9113e-03,\n",
      "         -4.8450e-03, -4.0188e-03, -4.6690e-04,  7.7994e-03,  1.8730e-03,\n",
      "          7.3541e-03,  5.1881e-03, -3.2609e-03,  6.0277e-04, -7.4914e-03,\n",
      "         -2.0049e-03,  5.9468e-03, -2.0220e-03,  7.1899e-03, -6.7874e-03,\n",
      "          9.7759e-04, -3.4398e-03, -2.9599e-03, -1.2971e-03, -3.0076e-03,\n",
      "          5.8829e-03, -2.9888e-04, -3.1608e-03,  4.6873e-03, -1.0073e-02,\n",
      "          2.6105e-03,  1.4135e-03,  1.7028e-03,  4.8198e-03, -8.5864e-04,\n",
      "          3.9043e-04, -2.9926e-03, -6.0480e-03, -3.4717e-03, -5.1876e-03,\n",
      "          3.3503e-03, -3.6559e-03,  3.6504e-03,  6.4670e-03,  1.1120e-03,\n",
      "          2.0567e-03,  5.8012e-04, -2.6472e-03,  4.5948e-03, -4.3540e-03,\n",
      "         -7.8023e-03,  5.9552e-03,  4.9466e-05,  2.2642e-03, -9.1550e-03,\n",
      "         -6.7902e-04, -6.1527e-03, -2.9867e-03, -6.2408e-03,  3.7379e-03,\n",
      "         -3.3005e-03,  1.3308e-04,  4.7967e-04, -2.0753e-03, -1.0793e-03,\n",
      "         -1.9021e-03, -1.6950e-03,  5.4223e-03, -5.7939e-03,  7.3355e-03,\n",
      "         -1.1190e-02, -4.9740e-04,  7.0711e-03,  1.8914e-03, -1.0869e-03,\n",
      "          6.2272e-03,  2.3717e-03,  3.9205e-04, -6.1054e-05,  9.1881e-04,\n",
      "          1.0464e-02, -3.3067e-03, -3.0153e-03,  2.6031e-03, -5.9792e-03,\n",
      "         -5.5047e-05,  6.8464e-04,  8.4277e-03, -5.5554e-03, -5.6707e-03,\n",
      "         -6.0481e-03,  2.4162e-04, -1.5591e-03,  3.4710e-03,  1.8065e-03,\n",
      "         -4.5479e-04, -4.2434e-03, -2.2052e-03,  3.7712e-03, -2.4609e-04,\n",
      "         -1.7803e-03,  2.8385e-03, -4.3449e-03,  3.8373e-03,  7.5980e-03,\n",
      "          1.0790e-03, -7.7901e-03,  2.4328e-03,  3.4619e-03, -5.4741e-03,\n",
      "          1.6679e-03,  1.0620e-02, -2.6431e-03,  1.9068e-03, -2.3350e-03,\n",
      "          3.9071e-03,  5.9490e-03,  7.9015e-04,  2.5520e-03,  2.4302e-03,\n",
      "          3.3008e-03, -4.8598e-03,  8.2042e-03, -2.5310e-03, -5.3529e-03,\n",
      "         -5.2466e-03, -2.7180e-03, -1.9922e-03,  8.0376e-03,  5.6360e-03,\n",
      "          3.3049e-03,  3.7872e-04, -1.7867e-03,  3.9080e-03,  5.7225e-03,\n",
      "         -4.6579e-03, -1.4774e-03,  1.9100e-03,  3.5890e-03,  9.2636e-04,\n",
      "         -2.1607e-03,  3.5993e-03, -2.2960e-03,  5.4168e-03, -3.6827e-03,\n",
      "          6.8257e-03, -6.0352e-03,  7.6601e-03, -4.9889e-03,  6.3007e-04,\n",
      "         -1.3572e-03,  4.1707e-04, -1.6681e-03,  5.9200e-04, -8.1154e-03,\n",
      "          1.1584e-02, -1.1950e-03,  7.3625e-03, -1.9847e-03,  7.7868e-03,\n",
      "          6.8491e-03,  3.1659e-03, -1.0216e-03,  4.2553e-03, -3.3589e-03,\n",
      "          1.0195e-02, -6.1488e-03, -1.0005e-03, -7.3639e-03, -1.7998e-04,\n",
      "         -5.8113e-03,  1.0270e-02,  5.6455e-03,  3.9155e-03, -7.8364e-04,\n",
      "         -5.6934e-03, -2.9341e-03, -3.9701e-03, -1.1513e-03, -2.4044e-03,\n",
      "         -8.6270e-03, -2.9571e-03,  4.4092e-03,  8.8284e-04, -1.5125e-03,\n",
      "         -1.4301e-03, -1.3414e-03,  2.3710e-04,  1.1989e-02]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(mel_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_out_np = mel_out.detach().numpy()\n",
    "mel_out_np = mel_out_np.reshape(128,3)\n",
    "np.save('data/mel_out_np_81.npy', mel_out_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mel_out_np, interpolation='nearest')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
